{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T21:56:32.427855Z",
     "start_time": "2019-10-26T21:56:32.320827Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, \n",
    "                              AdaBoostClassifier, BaggingRegressor)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.features import feature_extraction_VGGish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T19:18:12.022662Z",
     "start_time": "2019-10-26T19:18:11.950294Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T19:23:21.779171Z",
     "start_time": "2019-10-26T19:23:21.676958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/greenapple/project3/src/features/feature_extraction_VGGish.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '/Users/greenapple/project3/src/features/feature_extraction_VGGish.py'\n",
    "\n",
    "import os\n",
    "import soundfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from src.features import vggish_input\n",
    "from src.features import vggish_params\n",
    "from src.features import vggish_postprocess\n",
    "from src.features import vggish_slim\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "def read_audio(path, target_fs=None):\n",
    "    (audio, fs) = soundfile.read(path)\n",
    "\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "        \n",
    "    if target_fs is not None and fs != target_fs:\n",
    "        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)\n",
    "        fs = target_fs\n",
    "        \n",
    "    return audio, fs\n",
    "    \n",
    "\n",
    "# Feature extraction\n",
    "def extract_audioset_embedding(audio_file):\n",
    "    \"\"\"Extract log mel spectrogram features. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Arguments & parameters\n",
    "    mel_bins = vggish_params.NUM_BANDS\n",
    "    sample_rate = vggish_params.SAMPLE_RATE\n",
    "    input_len = vggish_params.NUM_FRAMES\n",
    "    embedding_size = vggish_params.EMBEDDING_SIZE\n",
    "    \n",
    "    '''You may modify the EXAMPLE_HOP_SECONDS in vggish_params.py to change the \n",
    "    hop size. '''\n",
    "\n",
    "    # Paths\n",
    "    path = '/Users/greenapple/project3/data/house_activities_wavs/'\n",
    "    audio_path = os.path.join(path, audio_file)\n",
    "    checkpoint_path = '/Users/greenapple/project3/src/features/vggish_model.ckpt'\n",
    "    pcm_params_path = '/Users/greenapple/project3/src/features/vggish_pca_params.npz'\n",
    "    \n",
    "    # Load model\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    vggish_slim.define_vggish_slim(training=False)\n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "    features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "    \n",
    "    pproc = vggish_postprocess.Postprocessor(pcm_params_path)\n",
    "\n",
    "    # Read audio\n",
    "    (audio, _) = read_audio(audio_path, target_fs=sample_rate)\n",
    "    \n",
    "    # Extract log mel feature\n",
    "    logmel = vggish_input.waveform_to_examples(audio, sample_rate)\n",
    "\n",
    "    # Extract embedding feature\n",
    "    [embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: logmel})\n",
    "    \n",
    "    # PCA\n",
    "    postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "    \n",
    "    print('Audio length: {}'.format(len(audio)))\n",
    "    print('Log mel shape: {}'.format(logmel.shape))\n",
    "    print('Embedding feature shape: {}'.format(postprocessed_batch.shape))\n",
    "    \n",
    "    return postprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T21:56:51.898529Z",
     "start_time": "2019-10-26T21:56:51.205512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained models\n",
    "model_names = [\n",
    "    'logreg1',\n",
    "    'logreg2',\n",
    "    'logreg3',\n",
    "    'KNN',\n",
    "    'NBmultinomial',\n",
    "    'RF',\n",
    "    'GBM'\n",
    "]\n",
    "\n",
    "model_fldr = '/Users/greenapple/project3/aws/models'\n",
    "\n",
    "for model in model_names:\n",
    "    pickling_out = open(os.path.join(model_fldr, f'{model}_10_cls_model_rand.pkl'), 'rb')\n",
    "    exec(f'{model} = pickle.load(pickling_out)')\n",
    "    pickling_out.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T21:57:10.718182Z",
     "start_time": "2019-10-26T21:57:10.618204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converts an audio file to a dataframe with features that are ready for a model prediction\n",
    "def audio_to_df(audio_file, audio_folder):\n",
    "    \n",
    "    # Extract features\n",
    "    audio_path = os.path.join(audio_folder, audio_file)\n",
    "    features = extract_audioset_embedding(audio_path)\n",
    "    \n",
    "    # Take date from the first 5 sec of the wav file\n",
    "    features_5sec = features[:5]\n",
    "    \n",
    "    # Reshape array in 1 row with 640 features\n",
    "    features_row = features_5sec.reshape(1, 640)\n",
    "    features_df = pd.DataFrame(features_row)\n",
    "    \n",
    "    return features_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T21:57:28.554131Z",
     "start_time": "2019-10-26T21:57:28.397814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Makes a dataframe with features extracted from the house activity audio files\n",
    "\n",
    "def audio_list_to_df(audio_file_list, audio_folder):\n",
    "    audio_df = pd.DataFrame()\n",
    "    for audio_file in audio_file_list:\n",
    "        row = audio_to_df(audio_file, audio_folder)\n",
    "        audio_df = pd.concat([audio_df, row], ignore_index=True)   \n",
    "    return audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T21:57:42.083719Z",
     "start_time": "2019-10-26T21:57:32.150616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 160128\n",
      "Log mel shape: (10, 96, 64)\n",
      "Embedding feature shape: (10, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 80214\n",
      "Log mel shape: (5, 96, 64)\n",
      "Embedding feature shape: (5, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 160086\n",
      "Log mel shape: (10, 96, 64)\n",
      "Embedding feature shape: (10, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 80214\n",
      "Log mel shape: (5, 96, 64)\n",
      "Embedding feature shape: (5, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 160150\n",
      "Log mel shape: (10, 96, 64)\n",
      "Embedding feature shape: (10, 128)\n"
     ]
    }
   ],
   "source": [
    "# List of home activities sound files for prediction\n",
    "audio_file_list = [\n",
    "    '20191025_073237.wav',\n",
    "    '20191025_073546.wav',\n",
    "    '20191025_073858.wav',\n",
    "    '20191025_074029.wav',\n",
    "    '20191025_074057.wav'\n",
    "]\n",
    "\n",
    "audio_folder = '/Users/greenapple/project3/data/house_activities_wavs/'\n",
    "\n",
    "audio_df = audio_list_to_df(audio_file_list, audio_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T21:58:27.186884Z",
     "start_time": "2019-10-26T21:58:27.071614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes in a list of sound files and models - returns predicted values\n",
    "\n",
    "def sound_recognition(audio_files, models):\n",
    "    audio_df_pred = pd.DataFrame()\n",
    "   \n",
    "    # Extract features\n",
    "    audio_df = audio_list_to_df(audio_files.values(), audio_folder)\n",
    "    \n",
    "    # Predict\n",
    "    for model in models.values():\n",
    "        row = model.predict(audio_df)\n",
    "        row = row.reshape(1, len(audio_files))\n",
    "        row = pd.DataFrame(row)\n",
    "        audio_df_pred = pd.concat([audio_df_pred, row], ignore_index=True)  \n",
    "     \n",
    "    # Rename columns with sound file names\n",
    "    audio_df_pred.columns = audio_files.keys()\n",
    "    \n",
    "    # Add a column with model names\n",
    "    audio_df_pred['model'] = models.keys()\n",
    "    \n",
    "     # Change column order\n",
    "    columns = audio_df_pred.columns.to_list()\n",
    "    columns = columns[-1:] + columns[:-1]\n",
    "    audio_df_pred = audio_df_pred[columns]\n",
    "    audio_df_pred\n",
    "    \n",
    "    return audio_df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sound class for house recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T22:38:06.603216Z",
     "start_time": "2019-10-26T22:37:54.852206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 160128\n",
      "Log mel shape: (10, 96, 64)\n",
      "Embedding feature shape: (10, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 80214\n",
      "Log mel shape: (5, 96, 64)\n",
      "Embedding feature shape: (5, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 160086\n",
      "Log mel shape: (10, 96, 64)\n",
      "Embedding feature shape: (10, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 80214\n",
      "Log mel shape: (5, 96, 64)\n",
      "Embedding feature shape: (5, 128)\n",
      "INFO:tensorflow:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow:INFO:Restoring parameters from /Users/greenapple/project3/src/features/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 160150\n",
      "Log mel shape: (10, 96, 64)\n",
      "Embedding feature shape: (10, 128)\n"
     ]
    }
   ],
   "source": [
    "# Predict sound class from a list of sound files\n",
    "models = {\n",
    "    'Logistic_regression1':logreg1,\n",
    "    'Logistic_regression2':logreg2,\n",
    "    'Logistic_regression3':logreg3,\n",
    "    'KNN':KNN,\n",
    "    'Naive_Bayes_multinomial':NBmultinomial,\n",
    "    'Random_Forest':RF,\n",
    "    'Gradient_boosting':GBM\n",
    "}\n",
    "\n",
    "audio_files = {\n",
    "    'clarinet':'20191025_073237.wav',\n",
    "    'stairs':'20191025_073546.wav',\n",
    "    'water':'20191025_073858.wav',\n",
    "    'human_cat_sound':'20191025_074029.wav',\n",
    "    'cat':'20191025_074057.wav'\n",
    "}\n",
    "\n",
    "audio_df_pred = sound_recognition(audio_files, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T22:38:11.007878Z",
     "start_time": "2019-10-26T22:38:10.894455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>clarinet</th>\n",
       "      <th>stairs</th>\n",
       "      <th>water</th>\n",
       "      <th>human_cat_sound</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic_regression1</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic_regression2</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic_regression3</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>KNN</td>\n",
       "      <td>music</td>\n",
       "      <td>microwave</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>microwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Naive_Bayes_multinomial</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>microwave</td>\n",
       "      <td>meow</td>\n",
       "      <td>speech</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>music</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  clarinet     stairs      water human_cat_sound  \\\n",
       "0     Logistic_regression1  clarinet       door  water_tap            meow   \n",
       "1     Logistic_regression2  clarinet       door  water_tap            meow   \n",
       "2     Logistic_regression3  clarinet       door  water_tap            meow   \n",
       "3                      KNN     music  microwave  water_tap       water_tap   \n",
       "4  Naive_Bayes_multinomial  clarinet  microwave       meow          speech   \n",
       "5            Random_Forest     music      music      music           music   \n",
       "6        Gradient_boosting     music  water_tap  water_tap          speech   \n",
       "\n",
       "         cat  \n",
       "0       meow  \n",
       "1       meow  \n",
       "2       meow  \n",
       "3  microwave  \n",
       "4       meow  \n",
       "5      music  \n",
       "6     speech  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification of recorded sounds\n",
    "audio_df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little voting classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes more than 8 hours to train a voting or meta- classifiers. Build a little voting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T22:52:39.207390Z",
     "start_time": "2019-10-26T22:52:39.111881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Little voting classifier\n",
    "def little_vote(df_pred):\n",
    "    pred = []\n",
    "    df_pred_vc = pd.DataFrame()\n",
    "    \n",
    "    for column in list(df_pred.columns):\n",
    "        if column=='model':\n",
    "            pred.append('little_VotingClassifier')\n",
    "        else:\n",
    "            pred.append(df_pred[column].mode()[0])\n",
    "    \n",
    "    pred_dict = dict(zip(list(df_pred.columns), pred))\n",
    "    \n",
    "    df_pred = df_pred.append(pred_dict, ignore_index=True)\n",
    "        \n",
    "    return df_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T22:52:59.482138Z",
     "start_time": "2019-10-26T22:52:59.306483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>clarinet</th>\n",
       "      <th>stairs</th>\n",
       "      <th>water</th>\n",
       "      <th>human_cat_sound</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Logistic_regression1</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Logistic_regression2</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic_regression3</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>KNN</td>\n",
       "      <td>music</td>\n",
       "      <td>microwave</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>microwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Naive_Bayes_multinomial</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>microwave</td>\n",
       "      <td>meow</td>\n",
       "      <td>speech</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Random_Forest</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Gradient_boosting</td>\n",
       "      <td>music</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>little_VotingClassifier</td>\n",
       "      <td>clarinet</td>\n",
       "      <td>door</td>\n",
       "      <td>water_tap</td>\n",
       "      <td>meow</td>\n",
       "      <td>meow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  clarinet     stairs      water human_cat_sound  \\\n",
       "0     Logistic_regression1  clarinet       door  water_tap            meow   \n",
       "1     Logistic_regression2  clarinet       door  water_tap            meow   \n",
       "2     Logistic_regression3  clarinet       door  water_tap            meow   \n",
       "3                      KNN     music  microwave  water_tap       water_tap   \n",
       "4  Naive_Bayes_multinomial  clarinet  microwave       meow          speech   \n",
       "5            Random_Forest     music      music      music           music   \n",
       "6        Gradient_boosting     music  water_tap  water_tap          speech   \n",
       "7  little_VotingClassifier  clarinet       door  water_tap            meow   \n",
       "\n",
       "         cat  \n",
       "0       meow  \n",
       "1       meow  \n",
       "2       meow  \n",
       "3  microwave  \n",
       "4       meow  \n",
       "5      music  \n",
       "6     speech  \n",
       "7       meow  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_df_pred_vc = little_vote(audio_df_pred)\n",
    "audio_df_pred_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T20:59:54.843814Z",
     "start_time": "2019-10-26T20:59:54.710795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate model train time. Explore later.\n",
    "from scitime import Estimator "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
